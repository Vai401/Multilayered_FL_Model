{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRFHTiEHbEqk"
      },
      "source": [
        "**Federated Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2Ia6C7lbJh0",
        "outputId": "25174d90-88ed-4f01-9090-e25914687c2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 0 | global_acc: 87.762% | global_loss: 1.676094651222229\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 1 | global_acc: 90.643% | global_loss: 1.6223770380020142\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 2 | global_acc: 91.667% | global_loss: 1.5997369289398193\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 3 | global_acc: 92.429% | global_loss: 1.5877500772476196\n",
            "132/132 [==============================] - 1s 4ms/step\n",
            "comm_round: 4 | global_acc: 92.667% | global_loss: 1.5782935619354248\n",
            "132/132 [==============================] - 1s 3ms/step\n",
            "comm_round: 5 | global_acc: 93.333% | global_loss: 1.5716933012008667\n",
            "132/132 [==============================] - 1s 4ms/step\n",
            "comm_round: 6 | global_acc: 93.500% | global_loss: 1.5661882162094116\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 7 | global_acc: 93.619% | global_loss: 1.5622512102127075\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 8 | global_acc: 93.976% | global_loss: 1.5575284957885742\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 9 | global_acc: 94.095% | global_loss: 1.5551342964172363\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 10 | global_acc: 94.214% | global_loss: 1.5523284673690796\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 11 | global_acc: 94.667% | global_loss: 1.5487468242645264\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 12 | global_acc: 94.690% | global_loss: 1.5471141338348389\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 13 | global_acc: 94.786% | global_loss: 1.5448569059371948\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 14 | global_acc: 94.857% | global_loss: 1.5433214902877808\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 15 | global_acc: 94.833% | global_loss: 1.5413304567337036\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 16 | global_acc: 95.143% | global_loss: 1.540001630783081\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 17 | global_acc: 95.167% | global_loss: 1.5386903285980225\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 18 | global_acc: 95.167% | global_loss: 1.5373121500015259\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 19 | global_acc: 95.214% | global_loss: 1.5360831022262573\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 20 | global_acc: 95.262% | global_loss: 1.535003423690796\n",
            "132/132 [==============================] - 1s 4ms/step\n",
            "comm_round: 21 | global_acc: 95.286% | global_loss: 1.5340319871902466\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 22 | global_acc: 95.381% | global_loss: 1.5337125062942505\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 23 | global_acc: 95.381% | global_loss: 1.5320593118667603\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 24 | global_acc: 95.429% | global_loss: 1.5313595533370972\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 25 | global_acc: 95.452% | global_loss: 1.5303810834884644\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 26 | global_acc: 95.500% | global_loss: 1.5300259590148926\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 27 | global_acc: 95.548% | global_loss: 1.529021143913269\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 28 | global_acc: 95.738% | global_loss: 1.5284628868103027\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 29 | global_acc: 95.667% | global_loss: 1.5283560752868652\n",
            "132/132 [==============================] - 1s 3ms/step\n",
            "comm_round: 30 | global_acc: 95.714% | global_loss: 1.5273292064666748\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 31 | global_acc: 95.714% | global_loss: 1.527009129524231\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 32 | global_acc: 95.810% | global_loss: 1.5262441635131836\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 33 | global_acc: 95.738% | global_loss: 1.5258551836013794\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 34 | global_acc: 95.833% | global_loss: 1.5250129699707031\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 35 | global_acc: 95.833% | global_loss: 1.5249416828155518\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 36 | global_acc: 95.952% | global_loss: 1.524041771888733\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 37 | global_acc: 95.762% | global_loss: 1.5237772464752197\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 38 | global_acc: 95.810% | global_loss: 1.5236107110977173\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 39 | global_acc: 95.881% | global_loss: 1.5228677988052368\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 40 | global_acc: 95.881% | global_loss: 1.5227746963500977\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 41 | global_acc: 95.905% | global_loss: 1.522135853767395\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 42 | global_acc: 96.024% | global_loss: 1.52214515209198\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 43 | global_acc: 95.857% | global_loss: 1.5216680765151978\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 44 | global_acc: 95.929% | global_loss: 1.5215537548065186\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 45 | global_acc: 95.976% | global_loss: 1.520824670791626\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 46 | global_acc: 95.976% | global_loss: 1.5203286409378052\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 47 | global_acc: 95.929% | global_loss: 1.5202751159667969\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 48 | global_acc: 95.929% | global_loss: 1.5199579000473022\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 49 | global_acc: 96.095% | global_loss: 1.5196337699890137\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 50 | global_acc: 96.119% | global_loss: 1.5192077159881592\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 51 | global_acc: 96.000% | global_loss: 1.5191434621810913\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 52 | global_acc: 96.167% | global_loss: 1.5187450647354126\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 53 | global_acc: 96.095% | global_loss: 1.518580675125122\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 54 | global_acc: 96.048% | global_loss: 1.518370270729065\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 55 | global_acc: 96.048% | global_loss: 1.5183298587799072\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 56 | global_acc: 96.095% | global_loss: 1.5181143283843994\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 57 | global_acc: 96.095% | global_loss: 1.5176923274993896\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 58 | global_acc: 96.214% | global_loss: 1.5172532796859741\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 59 | global_acc: 96.119% | global_loss: 1.517349123954773\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 60 | global_acc: 96.119% | global_loss: 1.517185926437378\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 61 | global_acc: 96.190% | global_loss: 1.516913652420044\n",
            "132/132 [==============================] - 1s 4ms/step\n",
            "comm_round: 62 | global_acc: 96.167% | global_loss: 1.516830325126648\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 63 | global_acc: 96.071% | global_loss: 1.5167455673217773\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 64 | global_acc: 96.119% | global_loss: 1.516329288482666\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 65 | global_acc: 96.286% | global_loss: 1.516063928604126\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 66 | global_acc: 96.262% | global_loss: 1.5159811973571777\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 67 | global_acc: 96.214% | global_loss: 1.5157694816589355\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 68 | global_acc: 96.071% | global_loss: 1.5157028436660767\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 69 | global_acc: 96.238% | global_loss: 1.5151742696762085\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 70 | global_acc: 96.167% | global_loss: 1.515217661857605\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 71 | global_acc: 96.190% | global_loss: 1.5149418115615845\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 72 | global_acc: 96.262% | global_loss: 1.5148977041244507\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 73 | global_acc: 96.238% | global_loss: 1.514770269393921\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 74 | global_acc: 96.310% | global_loss: 1.5146554708480835\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 75 | global_acc: 96.238% | global_loss: 1.5144749879837036\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 76 | global_acc: 96.310% | global_loss: 1.514401912689209\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 77 | global_acc: 96.310% | global_loss: 1.5141713619232178\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 78 | global_acc: 96.286% | global_loss: 1.5143991708755493\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 79 | global_acc: 96.333% | global_loss: 1.5139943361282349\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 80 | global_acc: 96.190% | global_loss: 1.5138365030288696\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 81 | global_acc: 96.286% | global_loss: 1.513726830482483\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 82 | global_acc: 96.190% | global_loss: 1.513500690460205\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 83 | global_acc: 96.310% | global_loss: 1.5133693218231201\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 84 | global_acc: 96.333% | global_loss: 1.5132352113723755\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 85 | global_acc: 96.214% | global_loss: 1.5130523443222046\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 86 | global_acc: 96.333% | global_loss: 1.5130473375320435\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 87 | global_acc: 96.381% | global_loss: 1.5128878355026245\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 88 | global_acc: 96.310% | global_loss: 1.5128391981124878\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 89 | global_acc: 96.357% | global_loss: 1.512759804725647\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 90 | global_acc: 96.310% | global_loss: 1.5126038789749146\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 91 | global_acc: 96.333% | global_loss: 1.5124878883361816\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 92 | global_acc: 96.357% | global_loss: 1.5124266147613525\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 93 | global_acc: 96.429% | global_loss: 1.5123413801193237\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 94 | global_acc: 96.357% | global_loss: 1.5120947360992432\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 95 | global_acc: 96.357% | global_loss: 1.5122591257095337\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 96 | global_acc: 96.357% | global_loss: 1.511841893196106\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 97 | global_acc: 96.357% | global_loss: 1.5118601322174072\n",
            "132/132 [==============================] - 0s 2ms/step\n",
            "comm_round: 98 | global_acc: 96.405% | global_loss: 1.5116424560546875\n",
            "132/132 [==============================] - 0s 3ms/step\n",
            "comm_round: 99 | global_acc: 96.333% | global_loss: 1.511637568473816\n"
          ]
        }
      ],
      "source": [
        "smlp_global = SimpleMLP()\n",
        "global_model = smlp_global.build(784, 10)\n",
        "loss=[]\n",
        "acc=[]\n",
        "#commence global training loop\n",
        "for comm_round in range(comms_round):\n",
        "\n",
        "    # get the global model's weights - will serve as the initial weights for all local models\n",
        "    global_weights = global_model.get_weights()\n",
        "\n",
        "    #initial list to collect local model weights after scalling\n",
        "    scaled_local_weight_list = list()\n",
        "\n",
        "    #randomize client data - using keys\n",
        "    client_names= list(clients_batched.keys())\n",
        "    random.shuffle(client_names)\n",
        "\n",
        "    #loop through each client and create new local model\n",
        "    for client in client_names:\n",
        "        smlp_local = SimpleMLP()\n",
        "        local_model = smlp_local.build(784, 10)\n",
        "        local_model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=optimizer,\n",
        "                      metrics=metrics)\n",
        "\n",
        "        #set local model weight to the weight of the global model\n",
        "        local_model.set_weights(global_weights)\n",
        "\n",
        "        #fit local model with client's data\n",
        "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
        "\n",
        "        #scale the model weights and add to list\n",
        "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
        "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "\n",
        "        #clear session to free memory after each communication round\n",
        "        K.clear_session()\n",
        "\n",
        "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
        "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "\n",
        "    #update global model\n",
        "    global_model.set_weights(average_weights)\n",
        "\n",
        "    #test global model and print out metrics after each communications round\n",
        "    for(X_test, Y_test) in test_batched:\n",
        "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n",
        "        loss.append(global_loss)\n",
        "        acc.append(global_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BETTdlf7jaBd",
        "outputId": "59ba72a1-1eb2-4d4e-aea1-b0aabf1c3c3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "global_model.save(\"F:/SegmentationModel/UNET/model-sdc-seg-v12.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YmuVKK4BjZPu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EkH5Ul4_hHxX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
